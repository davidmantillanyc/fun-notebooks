{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fishy Fun with Doc2Vec\n",
    "### Using a fishkeeping forum corpus with everyone's favorite vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to play around with word2vec but did not want to use the typical data sets (IMDB, etc.). So, I said, what if I were to do some web scraping of one of my favorite fishkeeping forums and attempt to apply word2vec to find \"experts\" within the forum.  Well, turns out this is a much longer journey than I originally thought it would be, but an interesting one nonetheless.\n",
    "\n",
    "This is a first blog post of hopefully several of my adventures with word2vec/doc2vec.  I have a few ideas on how to leverage this corpus using deep learning to auto-generate text, so stay tuned, and if interested, drop me a line or leave a comment!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "So word2vec was originally developed by Google researchers and many people have discussed the algorithm.  Word2vec provides a vector representation of a sequence of words using a not-deep neural network.  Doc2vec adds additional information (namely context, or paragraph context) to the word embeddings.  The original paper on Paragraph Vector can be found at https://cs.stanford.edu/~quocle/paragraph_vector.pdf   A quick literature search revealed I wanted to use doc2vec instead of word2vec for my particular use case since I wanted to compare user posts (essentially multiple paragaphs) instead of just words.\n",
    "\n",
    "Later, I found this very informative online video from PyData Berlin 2017 where another data scientist used doc2vec to analyze comments on news websites.  I thought that was cool, and further fueled my interest to  tinker with this algorithm in my spare time... fast forward a few hours, and its almost daylight and I'm still here typing away...\n",
    "\n",
    "I highly recommend watching this video for additional context: https://youtu.be/zFScws0mb7M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I'm trying to do\n",
    "I'd like to do the following:\n",
    "* analyze user posts on Fishlore.com to identify who are the \"experts\" on fishkeeping and plants/aquascaping\n",
    "* have fun with doc2vec while doing this\n",
    "\n",
    "Some challenges:\n",
    "* no corpus -> need to scrape the site myself (this was relatively easy to do w/ scrapy)\n",
    "* no training data -> there isn't a training set for who are \"experts\" on this site.  Many \"newbies\" have a handful of posts. Perhaps # posts could be a proxy for \"expert\" but there are some very prolific posters who are new to the hobby. \n",
    "* relatively small data set -> only ~17,000 users. A million or so posts (that I scraped). Might be good enough for fun, but its not Google scale here by any means\n",
    "* highly related documents -> they're almost all about fish! so same vocabulary, hard to differentiate.\n",
    "* highly specialized vocabulary -> many entities (cardinal tetra, lemon tetra, other fish and plant species, etc.) that may need to be encoded or tagged (or just let doc2vec figure it out w/ enough input data, right?)\n",
    "\n",
    "Let's have some fun and forgive me if I cut corners here... it was getting very late at night ;)\n",
    "\n",
    "### What happened\n",
    "Used gensim python library for word2vec/doc2vec functionality. I jumped right in, did some google searches trying to follow some tutorials. So, many of the tutorials I found were out of date (old gensim API). Of course since I was trying to do this as fast as possible, I didn't read thru the gensim code until much later. The tutorials online are out of date, the API has changed a bit.  The best resources were the sample ipython notebooks in the gensim library itself (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb), so if you are interested, follow those first, and forget the online blog posts (except mine since it's really current and my code actually runs ;) \n",
    "\n",
    "\n",
    "#### Pre-work:\n",
    "\n",
    "I scraped fishlore.com using scrapy.  I wrote some custom spiders to download threads in various forums areas, including:\n",
    "* beginners\n",
    "* plants\n",
    "* fish disease\n",
    "* other interests\n",
    "\n",
    "Then, I saved the output by username; that is, one file per user, with one line per user post.   This came out to about to 383MB uncompressed text.  This wasn't a complete scrap of fishlore.com, but I figured it was enough to have some interesting results and without them yelling at me. It yielded about posts by about 24,594 users.  I am tempted to create some plots/charts about the data set (# posts per user, etc.) but for now I decided to skip that since I really wanted to get to working with doc2vec.  I should mentioned I scraped the site over several days.\n",
    "\n",
    "Since I want to eventually find \"experts\" based on their posts, I define a document to be all the posts by a particular user.  There may be other/better ways to define a document (such as, by user-topic, i.e., Bob-plants, Bob-fishdisease, etc.) but I figured this was a reasonable first approach.\n",
    "\n",
    "\n",
    "### Implementation\n",
    "I followed the Doc2Vec-IMDB example (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb), and modified it for my corpus as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup python notebooks and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import glob\n",
    "import re\n",
    "from six import iteritems\n",
    "import itertools\n",
    "from collections import OrderedDict, namedtuple\n",
    "import multiprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "\n",
    "# Make sure we have Cython installed properly and we setup parallelism\n",
    "cores = multiprocessing.cpu_count()-1\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "# Turn on logging since it may take a while to train models\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def lookup_result(target):\n",
    "    \"\"\"\n",
    "    Let us lookup the user name given a document ID.\n",
    "    (numerical IDs are more memory efficient than string-based document tags)\n",
    "    \"\"\"\n",
    "    for u, doc_id_ in allusers_index.items():\n",
    "        if doc_id_ == target:\n",
    "            return u\n",
    "\n",
    "def preprocess_text(s):\n",
    "    \"\"\"\n",
    "    Perform some basic text cleanup\n",
    "    \"\"\"\n",
    "    s = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \n",
    "               \" _WEBSITE_ \", s) # replace web URLs  https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url\n",
    "    s = re.sub(r\"\"\"said.*Click to expand\\.\\.\\.\"\"\", \"  \", s) # get rid of quote blocks\n",
    "    s = re.sub(r\"\"\"\\$(([1-9]\\d{0,2}(,\\d{3})*)|(([1-9]\\d*)?\\d))(\\.\\d\\d)?$\"\"\", \n",
    "               \" _DOLLAR_AMOUNT_ \", s) # replace dollar amounts  https://stackoverflow.com/questions/17864213/java-regular-expression-to-match-dollar-amounts\n",
    "    s = re.sub(r\"\\.\\.\\.\", r\" . \", s) # convert ellipsis to period\n",
    "    s = re.sub(r\"\\.\\.\", r\" . \", s) # convert ellipsis to period\n",
    "    s = re.sub(r\"gallons\", r\"gallon\", s) # domain specific transformation\n",
    "    s = re.sub(r\"([,.!?:])\", r\" \\1 \", s) # convert sentence markers to words\n",
    "    s = re.sub(r\"\"\"[\\\"~\\(\\)]\"\"\",\"\", s) # get rid of unusual punctuation\n",
    "    s = s.lower() # convert to lowercase\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define `FishyDocument` as our document type. Notice it's just a `namedtuple`.  We'll load the user data, apply some  basic data transformations and tag the \"documents\" with a label.  Notice we're reading it all into memory.  Works OK for now, but not super scalable. I filtered out the documents for longer-length documents (at least 5000 words) to keep training speed fast.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim 3.2 uses TaggedDocuments which are actually namedtuples:\n",
    "FishyDocument = namedtuple('FishyDocument', 'words tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dir = \"/Users/david/git/data/fishlore/users\"\n",
    "user_files = glob.iglob(user_dir + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file #0\n",
      "Loading file #2000\n",
      "Loading file #4000\n",
      "Loading file #6000\n",
      "Loading file #8000\n",
      "Loading file #10000\n",
      "Loading file #12000\n",
      "Loading file #14000\n",
      "Loading file #16000\n",
      "Loading file #18000\n",
      "Loading file #20000\n",
      "Loading file #22000\n",
      "Loading file #24000\n",
      "Read in 24594 files. Used 1883 files\n"
     ]
    }
   ],
   "source": [
    "# Let's use the IMDB Doc2Vec example ipython notebook as a template\n",
    "# and customize for our example use case:\n",
    "allusers_index = {} # Will function as a lookup table so we can find the username corresponding to a doc_ID\n",
    "alldocs = []  # Will hold all docs in original order\n",
    "counter = 0\n",
    "for file_number, fn in enumerate(user_files):\n",
    "    username = fn.split(\"/\")[-1]\n",
    "    with open(fn) as f:\n",
    "        allwords = []\n",
    "        #tags = [file_number]\n",
    "        for line in f:\n",
    "            line = preprocess_text(line)\n",
    "            tokens = gensim.utils.to_unicode(line).split()\n",
    "            words = tokens[1:] \n",
    "            if len(words) > 5: # we dont want short posts like \"Bump\" or \"look great!\"\n",
    "                allwords.extend(words)\n",
    "        if len(allwords) > 5000: # let's look at longer documents for now\n",
    "            allusers_index[username] = counter\n",
    "            tags = [counter]\n",
    "            counter += 1\n",
    "            alldocs.append(FishyDocument(allwords, tags))\n",
    "    \n",
    "    if file_number%2000==0: print \"Loading file #{0}\".format(file_number)\n",
    "\n",
    "print \"Read in {0} files. Used {1} files\".format(file_number, counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some models. Here I'm again using the IMDB ipython notebook for inspiration / code reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-07 00:41:22,578 : INFO : collecting all words and their counts\n",
      "2018-01-07 00:41:22,580 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-01-07 00:41:31,427 : INFO : collected 205021 word types and 1883 unique tags from a corpus of 1883 examples and 40475453 words\n",
      "2018-01-07 00:41:31,428 : INFO : Loading a fresh vocabulary\n",
      "2018-01-07 00:41:32,935 : INFO : min_count=2 retains 81238 unique words (39% of original 205021, drops 123783)\n",
      "2018-01-07 00:41:32,936 : INFO : min_count=2 leaves 40351670 word corpus (99% of original 40475453, drops 123783)\n",
      "2018-01-07 00:41:33,195 : INFO : deleting the raw counts dictionary of 205021 items\n",
      "2018-01-07 00:41:33,212 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2018-01-07 00:41:33,214 : INFO : downsampling leaves estimated 28488881 word corpus (70.6% of prior 40351670)\n",
      "2018-01-07 00:41:33,215 : INFO : estimated required memory for 81238 words and 100 dimensions: 431314600 bytes\n",
      "2018-01-07 00:41:33,545 : INFO : using concatenative 1100-dimensional layer1\n",
      "2018-01-07 00:41:33,545 : INFO : resetting layer weights\n",
      "2018-01-07 00:41:34,660 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-07 00:41:35,762 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-07 00:41:36,809 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-07 00:41:38,301 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/m,d400,n5,w10,s0.001,t3)\n",
      "Doc2Vec(dbow,d400,n5,s0.001,t3)\n"
     ]
    }
   ],
   "source": [
    "simple_models = [\n",
    "    # PV-DM w/ concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/ average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "\n",
    "    # My attempts to do something different\n",
    "    Doc2Vec(min_count=1, window=10, size=400, sample=1e-3, negative=5, workers=cores),\n",
    "    Doc2Vec(min_count=1, window=10, size=400, sample=1e-3, negative=5, dm=0, workers=cores)\n",
    "]\n",
    "\n",
    "# Speed up setup by sharing results of the 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM w/ concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out some results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-07 00:53:35,039 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t3)\n",
      "target: TexasDomer\n",
      "\n",
      "most similar:\n",
      "\n",
      "deus ex machina\n",
      "CricketKeeper\n",
      "NighttHawk\n",
      "StephH\n",
      "midnightwolf\n",
      "ark_fish\n",
      "ASquidabs0727\n",
      "AtreyusMom\n",
      "Dave125g\n",
      "TheKiwi\n",
      "\n",
      "least similar:\n",
      "\n",
      "Scoutsfish\n",
      "MaeKay\n",
      "Norman\n",
      "CLam\n",
      "Scott2848\n",
      "Bhopkins1311\n",
      "Flowingfins\n",
      "fishnob\n",
      "skar\n",
      "Kenho21\n",
      "\n",
      "\n",
      "TARGET (1693): «googled it for you and found this : live leopard frog kit life cycle instructions - butterfly art and nature gifts - largest gift selection online ! leave her and the nest alone . also , bread is terrible for birds . i'd stop feeding her if she's wild so she's not reliant on you . if she's not wild , i'd get duck-specific feed . would leave all fish out . they will likely try to eat the fish , and as turtles are extremely messy , you wouldn't want to subject a fish to that . and with only 6-7 of water , there's not enough room for fish with the turtle . think they stay under 5 , from what i could find online . wouldn't . feeder fish like rosy minnows can carry who knows what . fish don't seem to make up much of their diet in the wild : common musk turtle care sheet maybe you're thinking of a 30 gal ? i don't know of any 20 gal long that is 36 , unless it's custom . fruits and veggies will work . from a google search : _website_ would chose a different light if you want a planted tank . that light is only good for the lowest of low light plants . do you have a budget for a light ? a glass lid would be a good idea for the tank too , instead of a hood . this way you can use a variety of lights , and it looks very nice ! i use versa glass tops . once you settle on a light , we can recommend plants . here's are some fish stocking options : _website_ don't know what that light is . this light is good for low light plants : _website_ you have lots of options for plants . jungle vals , swords , crypts , moneywort , water sprite , water lettuce , java fern , java moss , anubias , etc . than a hood , look for a glass lid . i like versa glass tops , but perfecto works too ! you do need to change water normally in a planted tank . both the fish and plants need fresh water with minerals . and depends on what you want . most people consider ramshorn and bladder snails to be pest snails . they breed rapidly and may eat your plants . malaysian trumpet snails stir up the substrate , but also breed quickly . don't think mts eat plants though someone correct me if i'm wrong ! , so you can leave them in your tank if you wanted . they don't do much harm . cories won't work in a 10 gal . they need to be in larger groups , and they're too active for a 10 gal . those tetra species won't work , but see the species listed here : _website_ quick growing plants , i recommend water wisteria and jungle val you can try to keep these short . both are fast growing and pretty ! water wisteria won't require root tabs either , as they feed from the water column . depends on the species , but many will not . they're super slow growers though . hornwort is also super fast , as is anacharis . pest snail eggs . they can also be used to feed your puffer . don't lay eggs; they're livebearers . that an abf ? it's too large for a 10 gal , but i think we've had this convo before . it'd appreciate some surface cover . 80 is fine for him ! _website_ the temp of the bulb ? usually 5000-10000 k will grow low light plants , though 6500 k is ideal . how many watts ? should be fine for low light plants . can be acclimated to excel use - you start with small doses of excel daily and work your way up to the full dose over a few weeks/months . however , it's not necessary for you to use excel . why are you adding ecocomplete after 3 months ? have you already added the fluval stratum and platinum soil ? if not , it's going to be very expensive for such a large tank . dirt with a gravel/sand cap , all ecocomplete , or sand with root tabs would be much cheaper and fine for your low tech , low light plants . you start noticing some deficiencies , you may want to dose dry ferts cheaper than liquid ferts for macros and micros . don't know of any available in india , sorry to fishlore ! rehome the pleco and the fiddler crab - the pleco will get huge and outgrow your tank even likely the tank you're going to upgrade to . the fiddler crab needs brackish water and a setup with land . you likelyhave lots of options for plants . what kind of lighting do you have ? java fern and anubias are good , hardly low light plants . don't plant them in the substrate - they can float or you can tie them to driftwood or rock . should return the pleco and the fiddler crab to your lfs . you can order plants online too . lfs might still take him though . if not , try to find another home for him . he is not appropriate for your tank size , and keeping him in a tank that's too small is cruel . lot can happen when you're not watching . is an old method of measuring lighting for plants . par ratings are currently the best measurement . a finnex planted plus would put you at low/medium to medium lighting , while a stingray would put you at low lighting . can try to find the par ratings of specific fixtures and bulbs online . if it were»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t3):\n",
      "\n",
      "MOST (494, 0.37302184104919434): «rubber banding is a good way to anchor plants to decorations then ? cause i was thinking of using thread . unless i could find some fishing line . have pebble like gravel at the top of my substrate in my 29 gallon , i have managed to plant 4 java fern by pinning it between the pebbles and not burying into the rest of the substrate , they're holding well . i just like to know whether thats fine to do ? the top half and is completely exposed , only the low roots have been covered because of the bottom of the substrate pressing against it . am i too lightly planted with only 4 java fern , no fish yet ? how many more plants [low light like fern , moss , anubias] can i add to be considered well stocked in the plant department . and should i go with the tablets from api for fertilizing ? or use liquid fert like leaf zone from api ? i have a t8 20 watt life glo bulb at 6700k . no co2 , prefer slow plant growth . also been thinking of making a coconut cave with java moss , basically making a h»\n",
      "\n",
      "MEDIAN (1364, -0.0007365243509411812): «dogs and 2 hamsters . i have had every animal you can legally own in the city , also had a horse we boarded out east a bit . currently have 6 tanks , 4 betta tanks , a female platy tank and a goldfish tank . my kids really , really want a leopard gecko and i do too , but i told them they have to wait till they are older as i don't think they are quite ready to care for one yet . well that was a fun little experiment . for some reason they don't upload the same from the laptop as the phone . oh well . am wanting to add an amazon sword plant to my goldfish tank , but i have never tried keeping live plants other than marimo moss balls and i have tiny little anubias just floating in my breeder box in another tank . i don't want to have to use fertilizers and all that so would the waste produced by my goldfish be enough to keep an amazon sword plant healthy ? i plan on planting it in a little clay flower pot with gravel . is a pic of the tank i'd like to put it in . i don't really have any »\n",
      "\n",
      "LEAST (1538, -0.37402719259262085): «duck has a leg issue on one leg and my rooster is having well . serious foot and toe issues . please note my animals are very well taken care of and they are all pets , even down to the chickens , ducks and roosters . they live in a large dirt pen that used to be covered but was taken down due to us having to expand the run , and the chicken wire roof was coming down anyway . there are 3 houses , two small wood tables for shade and a kiddie pool for summer . several water containers and a food bowl . theyre fed chicken scratch and were recently switched to chicken crumbles from ifa to the walmart brand due to it being cheaper . i'll start with my duck . i have 7 ducks , 2 indian runner ducks , another is a khaki campbell male theyre 3years now i believe , maybe more , maybe less . the other four are a year in june , babies of my female indian and male khaki . my male indian runner has had issues with his left leg for a couple years now . he has issues putting weight on it , and at time»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "doc_id = allusers_index['TexasDomer']\n",
    "#doc_id = allusers_index['bigdreams']\n",
    "#doc_id = allusers_index['jenmur']\n",
    "#doc_id = allusers_index['Lchi87']\n",
    "\n",
    "\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "\n",
    "print \"model\", model\n",
    "print \"target:\",\n",
    "print lookup_result(doc_id)\n",
    "print \n",
    "print \"most similar:\\n\"\n",
    "for i in range(10):\n",
    "    print lookup_result(sims[i][0])\n",
    "    \n",
    "print \"\\nleast similar:\\n\"\n",
    "for i in range(10):\n",
    "    print lookup_result(sims[len(sims) - 1 - i][0])\n",
    "\n",
    "print\n",
    "print\n",
    "\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words[:1000])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I happen to know `TexasDomer` is an expert on fishlore.com.  She is very knowledgeable about fishkeeping/husbandry, plants, and fish disease.  Here is an excerpt from her document:\n",
    "\n",
    "> here's are some fish stocking options : _website_ don't know what that light is . this light is good for low light plants : _website_ you have lots of options for plants . jungle vals , swords , crypts , moneywort , water sprite , water lettuce , java fern , java moss , anubias , etc . than a hood , look for a glass lid . i like versa glass tops , but perfecto works too ! you do need to change water normally in a planted tank . both the fish and plants need fresh water with minerals . and depends on what you want . most people consider ramshorn and bladder snails to be pest snails . they breed rapidly and may eat your plants . malaysian trumpet snails stir up the substrate , but also breed quickly . don't think mts eat plants though someone correct me if i'm wrong !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most similar document yielded reasonable results:\n",
    "\n",
    "> rubber banding is a good way to anchor plants to decorations then ? cause i was thinking of using thread . unless i could find some fishing line . have pebble like gravel at the top of my substrate in my 29 gallon , i have managed to plant 4 java fern by pinning it between the pebbles and not burying into the rest of the substrate , they're holding well . i just like to know whether thats fine to do ? the top half and is completely exposed , only the low roots have been covered because of the bottom of the substrate pressing against it . am i too lightly planted with only 4 java fern , no fish yet ? \n",
    "\n",
    "The least similar document was more impressive:\n",
    "\n",
    "> duck has a leg issue on one leg and my rooster is having well . serious foot and toe issues . please note my animals are very well taken care of and they are all pets , even down to the chickens , ducks and roosters . they live in a large dirt pen that used to be covered but was taken down due to us having to expand the run , and the chicken wire roof was coming down anyway . there are 3 houses , two small wood tables for shade and a kiddie pool for summer . several water containers and a food bowl . theyre fed chicken scratch and were recently switched to chicken crumbles from ifa to the walmart brand due to it being cheaper . i'll start with my duck . i have 7 ducks , 2 indian runner ducks , another is a khaki campbell male theyre 3years now i believe\n",
    "\n",
    "The least similar document seems to have a lot of posts from the \"other interests\" and \"other pets\" subforums.  So yes, ducks are not similar to fish ;)  More importantly perhaps, `TexasDomer` writes about different topics than `Scoutsfish`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "Before we can declare victory though, it is important to note that garbage in, garbage out applies here.  While the initial results are fun, not sure we can really prove anything. It's more entertainment value at this point.\n",
    "\n",
    "I'd like to continue and attempt to fit a machine learning model on this data to see if we can use the word embeddings to predict who an expert is. However, we do not have trained data for this, so not sure how feasible this really is. I'm going to spend the next several days/weeks thinking about this, assuming I have some spare time.  Please comment/write me if you found this interesting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
